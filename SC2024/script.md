# Intro

# Intake-esgf

- I would like to talk a moment about the programmatic interface that we have to our ESGF holdings. This we have implemented in a package that we call intake-esgf that runs in python.
- ESGF holdings are vast and so our catalogs initialize empty. They are populated by a search for keywords. In this case we are searching for a Canadian model that was run from 1850 to 2015. We are looking for monthly mean output for few variables: gross primary productivty, surface air temperature, and precipitation. This search queries a configurable set of indices and returns merged results in the form of a pandas dataframe.
- This dataframe the user can directly access. It is just a member of the catalog object itself. The users can then manipulate that by reducing rows and deciding what is the data they would like to download and return as datasets.
- Alternatively you can just print the catalog and what you get out of that are the columns of that dataframe and the unique values. This you can use to help you refine your search. 
- In this case we realize we didn't really specify which ensemble member that we want and so we add that to our last search and now we have 3 results which is what we had in mind and now are ready download.
- And so when you do this you launch a call cat.to_dataset_dict() which will return a dictionary of xarray datasets to you. Internally we are communicating again with the index nodes and looking for all the places where the files associated with those datasets are located. We will also automatically associate important auxillary data like cell measures.
- So when you ask for to_dataset_dict(), many things will happen under the hood. First, if the ile is found locally either because you have downloaded it before or because maybe you are working on a server resource where there is direct access to the data, then we ill prefer those local paths. If you indicate by keyword that you prefer to use streaming links and those links are available, for now we have OPENDAP links in the record, but this is expandable to other technologies like Zarr and Kerchunk, then we will prefer those. If you prefer to use globus as your transfer method then you only need to add into your keyword search a globus endpoint, the destination to which you would like to transfer data. If so and if information is present we will use globus to transfer data. As a last resort we will download the remaining files using a threaded https transfer.
- And so it returns data to you in the form of a dictionary where the keys are formed from all the facet values which are different in your current search. In this case it needs Amon and tas and you see when you reference that you get the surface air temperature with its areacella, the cell measures, returned in there also.
- Here we show just how you can use this basically to do some basic plotting. In this case we are taking the three things we were looking for, gross primary productivity, precipitation and surface air temperature, and we put them into plots using matplotlib. These are just directly accessible to you.
- Just a few key points as we finish up here. The programmatic nature of the interface is important because it allows the user to have a conversation with the index and understand what is there and available to them. It also does a very good job of hiding a lot of the ugly parts of data access and we tried to make that seamless for the user so they don't need to know how the data is organized in order to get it optimally. Where possible we are automating a lot of tasks which for the user is really painful. The interface is also really great because it unifies scripts that a user may run locally with some that we may run and access remotely in server-side compute applications.

# Rooki

# Globus Compute